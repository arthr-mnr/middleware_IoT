import asyncio
import os
from contextlib import AsyncExitStack
from typing import Optional
import google.generativeai as genai
from dotenv import load_dotenv

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Charger les variables d'environnement avec gestion d'erreur
try:
    load_dotenv()
except UnicodeDecodeError:
    print("âš ï¸  Erreur de lecture du fichier .env (problÃ¨me d'encodage)")
    print("RecrÃ©ez le fichier .env en UTF-8 sans BOM avec cette commande :")
    print('Set-Content -Path .env -Value "GOOGLE_API_KEY=votre_clÃ©" -Encoding UTF8')
    exit(1)

class MCPClient:
    def __init__(self):
        # Initialiser les sessions
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        # Configurer Gemini (accepte GEMINI_API_KEY ou GOOGLE_API_KEY)
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("\nâŒ ERREUR : ClÃ© API manquante !")
            print("\nCrÃ©ez un fichier .env avec :")
            print("GOOGLE_API_KEY=votre_clÃ©_api_ici")
            print("\nOu obtenez une clÃ© gratuite sur : https://aistudio.google.com/app/apikey")
            exit(1)
        
        genai.configure(api_key=api_key)
        
        # Liste des modÃ¨les Ã  essayer (noms corrects basÃ©s sur votre API)
        models_to_try = [
            'gemini-2.5-flash-lite',
            'gemini-2.5-flash',
            'gemini-2.0-flash-lite',
            'gemini-2.0-flash',
        ]
        
        # Essayer chaque modÃ¨le jusqu'Ã  en trouver un qui fonctionne
        self.model = None
        for model_name in models_to_try:
            try:
                print(f"ğŸ”„ Tentative avec le modÃ¨le : {model_name}")
                self.model = genai.GenerativeModel(model_name)
                # Test rapide pour vÃ©rifier que le modÃ¨le fonctionne
                test_response = self.model.generate_content("test")
                print(f"âœ… ModÃ¨le {model_name} chargÃ© avec succÃ¨s!")
                break
            except Exception as e:
                print(f"âš ï¸  {model_name} non disponible : {str(e)[:100]}")
                continue
        
        if self.model is None:
            print("\nâŒ ERREUR : Aucun modÃ¨le Gemini disponible!")
            print("VÃ©rifiez votre clÃ© API et votre connexion internet.")
            exit(1)
        
        self.tools: list = []

    async def connect_to_server(self, server_script_path: str):
        """Connexion au serveur MCP"""
        
        # DÃ©terminer le type de serveur
        is_python = server_script_path.endswith(".py")
        is_node = server_script_path.endswith(".js")
        
        if not (is_python or is_node):
            raise ValueError("Le serveur doit Ãªtre un fichier .py ou .js")
        
        # Configurer les paramÃ¨tres du serveur
        command = "python" if is_python else "node"
        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=None
        )
        
        # Connexion au serveur
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
        )
        
        await self.session.initialize()
        
        # Lister les outils disponibles
        response = await self.session.list_tools()
        self.tools = response.tools
        
        print("\nâœ… ConnectÃ© au serveur!")
        print(f"ğŸ”§ Outils disponibles: {[tool.name for tool in self.tools]}")

    def convert_tools_to_gemini_format(self):
        """Convertir les outils MCP au format Gemini"""
        gemini_tools = []
        
        for tool in self.tools:
            gemini_tool = {
                "function_declarations": [{
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }]
            }
            gemini_tools.append(gemini_tool)
        
        return gemini_tools

    async def process_query(self, query: str) -> str:
        """Traiter une requÃªte avec Gemini et les outils MCP"""
        
        # CrÃ©er le chat avec les outils
        gemini_tools = self.convert_tools_to_gemini_format()
        chat = self.model.start_chat(enable_automatic_function_calling=False)
        
        # Envoyer la requÃªte initiale
        response = chat.send_message(
            query,
            tools=gemini_tools if gemini_tools else None
        )
        
        # Traiter les appels de fonction
        max_iterations = 5
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            # VÃ©rifier s'il y a des candidats et des parts
            if not response.candidates:
                break
            
            candidate = response.candidates[0]
            if not hasattr(candidate, 'content') or not candidate.content:
                break
            
            # VÃ©rification plus robuste des parts
            if not hasattr(candidate.content, 'parts'):
                break
            
            parts = candidate.content.parts
            if parts is None or len(parts) == 0:
                break
                
            # Extraire les appels de fonction de maniÃ¨re sÃ©curisÃ©e
            function_calls = []
            for part in parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part)
            
            if not function_calls:
                break
            
            # ExÃ©cuter tous les appels de fonction
            function_responses = []
            
            for function_call in function_calls:
                tool_name = function_call.function_call.name
                tool_args = dict(function_call.function_call.args)
                
                print(f"\nğŸ”§ Appel de l'outil: {tool_name}")
                print(f"   Arguments: {tool_args}")
                
                # ExÃ©cuter l'outil via MCP
                result = await self.session.call_tool(tool_name, tool_args)
                
                # Extraire le texte du rÃ©sultat
                result_text = ""
                if result.content:
                    for content in result.content:
                        if hasattr(content, 'text'):
                            result_text += content.text
                
                function_responses.append(
                    genai.protos.Part(
                        function_response=genai.protos.FunctionResponse(
                            name=tool_name,
                            response={"result": result_text}
                        )
                    )
                )
            
            # Envoyer les rÃ©sultats Ã  Gemini
            response = chat.send_message(function_responses)
        
        # Extraire la rÃ©ponse finale de maniÃ¨re sÃ©curisÃ©e
        if response and hasattr(response, 'text') and response.text:
            final_response = response.text
        elif response and response.candidates:
            # Fallback pour extraire le texte manuellement
            final_response = ""
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and candidate.content and hasattr(candidate.content, 'parts'):
                    parts = candidate.content.parts
                    if parts:
                        for part in parts:
                            if hasattr(part, 'text'):
                                final_response += part.text
        else:
            final_response = "DÃ©solÃ©, je n'ai pas pu gÃ©nÃ©rer une rÃ©ponse."
        
        return final_response

    async def chat_loop(self):
        """Boucle de chat interactive"""
        print("\nğŸ¤– Client MCP avec Gemini dÃ©marrÃ©!")
        print("ğŸ’¡ Exemples de questions :")
        print("   - PrÃ©visions mÃ©tÃ©o pour New York")
        print("   - Alertes mÃ©tÃ©o en Californie (CA)")
        print("\nTape 'quit' pour quitter\n")
        
        while True:
            try:
                query = input("Vous: ").strip()
                
                if query.lower() == "quit":
                    print("\nğŸ‘‹ Au revoir !")
                    break
                
                if not query:
                    continue
                
                response = await self.process_query(query)
                print(f"\nğŸ’¬ Gemini: {response}\n")
                
            except Exception as e:
                print(f"\nâŒ Erreur: {str(e)}\n")

    async def cleanup(self):
        """Nettoyer les ressources"""
        await self.exit_stack.aclose()

async def main():
    if len(os.sys.argv) < 2:
        print("âŒ Usage: python client.py <chemin_vers_serveur.py>")
        print("\nğŸ“ Exemple:")
        print("   python client.py ../weather-server-python/weather.py")
        print("\nğŸ”‘ Assurez-vous d'avoir un fichier .env avec:")
        print("   GOOGLE_API_KEY=votre_clÃ©_api")
        return
    
    server_script = os.sys.argv[1]
    
    client = MCPClient()
    
    try:
        await client.connect_to_server(server_script)
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    import sys
    os.sys = sys
    asyncio.run(main())